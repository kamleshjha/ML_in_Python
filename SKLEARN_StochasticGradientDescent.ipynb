{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQVSB+SbwPiAq7HvxdAgtK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Stochastic Gradient Descent\n","\n"," Stochastic Gradient Descent (SGD) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.\n","\n"],"metadata":{"id":"Y6yMLgOF1dyG"}},{"cell_type":"markdown","source":["### SGD Classifier\n","Stochastic Gradient Descent (SGD) classifier basically implements a plain SGD learning routine supporting various loss functions and penalties for classification. Scikit-learn provides SGDClassifier module to implement SGD classification.\n"],"metadata":{"id":"lZNTaa-J1rKR"}},{"cell_type":"markdown","source":["#### Implementation Example\n","Like other classifiers, Stochastic Gradient Descent (SGD) has to be fitted with following two arrays.\n","1.   An array X holding the training samples. It is of size [n_samples, _features].\n","2.   An array Y holding the target values i.e. class labels for the training samples. It is of size [n_samples].\n","Following Python script uses SGDClassifier linear model"],"metadata":{"id":"LV7xMy603MQ5"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"lfVEBFNV1TOL","executionInfo":{"status":"ok","timestamp":1687701845155,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"84bb917d-c4c4-418c-85fe-9a4c687c0988"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SGDClassifier(penalty='elasticnet')"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(penalty=&#x27;elasticnet&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(penalty=&#x27;elasticnet&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":1}],"source":["import numpy as np\n","from sklearn import linear_model\n","X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n","Y = np.array([1, 1, 2, 2])\n","SGDClf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3,penalty=\"elasticnet\")\n","SGDClf.fit(X, Y)"]},{"cell_type":"code","source":["SGDClf.predict([[2.,2.]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kjf0TuLx3qbM","executionInfo":{"status":"ok","timestamp":1687701906126,"user_tz":-330,"elapsed":505,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"56c25dc1-c94e-49e3-fc6a-5cc6a16a84ed"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["### SGD Regressor\n","\n","Stochastic Gradient Descent (SGD) regressor basically implements a plain SGD learning routine supporting various loss functions and penalties to fit linear regression models. Scikit-learn provides SGDRegressor module to implement SGD regression"],"metadata":{"id":"b_aSK-aY36uW"}},{"cell_type":"markdown","source":["##### Parameters\n","Parameters used by SGDRegressor are almost same as that were used in SGDClassifier module. The difference lies in ‘loss’ parameter. For SGDRegressor modules’ loss parameter the positives values are as follows:\n","1. squared_loss: It refers to the ordinary least squares fit.huber: SGDRegressor correct the outliers by switching from squared to linear loss past a distance of epsilon. The work of ‘huber’ is to modify ‘squared_loss’ so that algorithm focus less on correcting outliers.\n","2. epsilon_insensitive: Actually, it ignores the errors less than epsilon.\n","3. squared_epsilon_insensitive: It is same as epsilon_insensitive. The only difference is that it becomes squared loss past a tolerance of epsilon.\n","Another difference is that the parameter named ‘power_t’ has the default value of 0.25 rather than 0.5 as in SGDClassifier. Furthermore, it doesn’t have ‘class_weight’ and ‘n_jobs’ parameters.\n","Attributes\n","Attributes of SGDRegressor are also same as that were of SGDClassifier module. Rather it has three extra attributes as follows:\n","4. average_coef_: array, shape(n_features,)\n","As name suggest, it provides the average weights assigned to the features.\n","5. average_intercept_: array, shape(1,)\n","As name suggest, it provides the averaged intercept term.\n","6. t_: int\n","It provides the number of weight updates performed during the training phase.\n","Note: the attributes average_coef_ and average_intercept_ will work after enabling parameter ‘average’ to True."],"metadata":{"id":"YEvzIV0M3-Fb"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn import linear_model\n","n_samples, n_features = 10, 5\n","rng = np.random.RandomState(0)\n","y = rng.randn(n_samples)\n","X = rng.randn(n_samples, n_features)\n","SGDReg =linear_model.SGDRegressor(max_iter=1000,penalty=\"elasticnet\",loss='huber',tol=1e-3, average=True)\n","SGDReg.fit(X, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"QS-VbPSA4feT","executionInfo":{"status":"ok","timestamp":1687702178366,"user_tz":-330,"elapsed":922,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"9b7d9849-7f56-4cc6-ed3f-f09c278c5e7c"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SGDRegressor(average=True, loss='huber', penalty='elasticnet')"],"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(average=True, loss=&#x27;huber&#x27;, penalty=&#x27;elasticnet&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(average=True, loss=&#x27;huber&#x27;, penalty=&#x27;elasticnet&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Pros and Cons of SGD\n","\n","**Following the pros of SGD:**\n","1. Stochastic Gradient Descent (SGD) is very efficient.\n","2. It is very easy to implement as there are lots of opportunities for code tuning\n","\n","**Following the cons of SGD:**\n","1. Stochastic Gradient Descent (SGD) requires several hyperparameters like regularization parameters.\n","2. It is sensitive to feature scaling."],"metadata":{"id":"-E-3OfTy5D4K"}},{"cell_type":"code","source":[],"metadata":{"id":"P1x6EIB245TU"},"execution_count":null,"outputs":[]}]}