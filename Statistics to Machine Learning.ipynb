{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Chapter 1 - Journey from Statistics to Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Setting up a Directory for Data file location\n",
    "os.chdir(\"D:/study/Statistics-for-Machine-Learning/Chapter01/Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basis Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 4.33\n",
      "Median : 4.0\n",
      "Mode : 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "data = np.array([4,5,1,2,7,2,6,9,3])\n",
    "\n",
    "# Calculate Mean\n",
    "dt_mean = np.mean(data) ; print (\"Mean :\",round(dt_mean,2))\n",
    "              \n",
    "# Calculate Median                 \n",
    "dt_median = np.median(data) ; print (\"Median :\",dt_median)        \n",
    "\n",
    "# Calculate Mode                     \n",
    "dt_mode =  stats.mode(data); print (\"Mode :\",dt_mode[0][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample variance: 400\n",
      "Sample std.dev: 20.0\n",
      "Range: 69\n"
     ]
    }
   ],
   "source": [
    "# Deviance calculations\n",
    "\n",
    "import numpy as np\n",
    "from statistics import variance,stdev\n",
    "\n",
    "game_points = np.array([35,56,43,59,63,79,35,41,64,43,93,60,77,24,82])\n",
    "\n",
    "# Calculate Variance\n",
    "dt_var = variance(game_points) ; print (\"Sample variance:\", round(dt_var,2))\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "dt_std = stdev(game_points) ; print (\"Sample std.dev:\",round(dt_std,2))\n",
    "               \n",
    "# Calculate Range\n",
    "dt_rng = np.max(game_points,axis=0) - np.min(game_points,axis=0) ; print (\"Range:\",dt_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles:\n",
      "20% 39.800000000000004\n",
      "80% 77.4\n",
      "100% 93.0\n",
      "Inter quartile range: 28.5\n"
     ]
    }
   ],
   "source": [
    "#Calculate percentiles\n",
    "print (\"Quantiles:\")\n",
    "for val in [20,80,100]:\n",
    "    dt_qntls = np.percentile(game_points,val) \n",
    "    print (str(val)+\"%\" ,dt_qntls)\n",
    "                                \n",
    "# Calculate IQR                           \n",
    "q75, q25 = np.percentile(game_points, [75 ,25]); print (\"Inter quartile range:\",q75-q25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: -4.38\n",
      "Critical value from t-table: -1.699\n",
      "Lower tail p-value from t-table 7.035025729010886e-05\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats              \n",
    "\n",
    "xbar = 990; mu0 = 1000; s = 12.5; n = 30\n",
    "# Test Statistic\n",
    "t_smple  = (xbar-mu0)/(s/np.sqrt(float(n))); print (\"Test Statistic:\",round(t_smple,2))\n",
    "# Critical value from t-table\n",
    "alpha = 0.05\n",
    "t_alpha = stats.t.ppf(alpha,n-1); print (\"Critical value from t-table:\",round(t_alpha,3))          \n",
    "#Lower tail p-value from t-table                        \n",
    "p_val = stats.t.sf(np.abs(t_smple), n-1); print (\"Lower tail p-value from t-table\", p_val)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. to score more than 67 is  17.87 %\n"
     ]
    }
   ],
   "source": [
    "# Normal Distribution\n",
    "from scipy import stats\n",
    "xbar = 67; mu0 = 52; s = 16.3\n",
    "\n",
    "# Calculating z-score\n",
    "z = (67-52)/16.3\n",
    "\n",
    "# Calculating probability under the curve    \n",
    "p_val = 1- stats.norm.cdf(z)\n",
    "print (\"Prob. to score more than 67 is \",round(p_val*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value is:  0.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:822: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  retval = getattr(retval, self.name)._getitem_axis(key, axis=i)\n"
     ]
    }
   ],
   "source": [
    "# Chi-square independence test\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "survey = pd.read_csv(\"survey.csv\")  \n",
    "# Tabulating 2 variables with row & column variables respectively\n",
    "survey_tab = pd.crosstab(survey.Smoke, survey.Exer, margins = True)\n",
    "# Creating observed table for analysis\n",
    "observed = survey_tab.ix[0:4,0:3] \n",
    "\n",
    "contg = stats.chi2_contingency(observed= observed)\n",
    "p_value = round(contg[1],3)\n",
    "print (\"P-value is: \",p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic : 3.66 , p-value : 0.051\n"
     ]
    }
   ],
   "source": [
    "#ANOVA\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "fetilizers = pd.read_csv(\"fetilizers.csv\")\n",
    "\n",
    "one_way_anova = stats.f_oneway(fetilizers[\"fertilizer1\"], fetilizers[\"fertilizer2\"], fetilizers[\"fertilizer3\"])\n",
    "\n",
    "print (\"Statistic :\", round(one_way_anova[0],2),\", p-value :\",round(one_way_anova[1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test split\n",
    "import pandas as pd      \n",
    "from sklearn.model_selection import train_test_split              \n",
    "                                \n",
    "original_data = pd.read_csv(\"mtcars.csv\")     \n",
    "\n",
    "train_data,test_data = train_test_split(original_data,train_size = 0.7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results\n",
      "Intercept 30.098860539622496 Coefficient [-0.06822828]\n",
      "Converged, iterations:  1142616\n",
      "Gradient Descent Results\n",
      "Intercept = [30.02495125] Coefficient = [-0.06781243]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regressio vs. Gradient Descent             \n",
    "               \n",
    "import numpy as np                        \n",
    "import pandas as pd\n",
    "                       \n",
    "train_data = pd.read_csv(\"mtcars.csv\")                       \n",
    "                        \n",
    "X = np.array(train_data[\"hp\"])  ; y = np.array(train_data[\"mpg\"]) \n",
    "X = X.reshape(32,1); y = y.reshape(32,1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept = True) \n",
    " \n",
    "model.fit(X,y)       \n",
    "print (\"Linear Regression Results\")        \n",
    "print (\"Intercept\",model.intercept_[0] ,\"Coefficient\",model.coef_[0])   \n",
    "                   \n",
    "\n",
    "def gradient_descent(x, y,learn_rate, conv_threshold,batch_size,max_iter):    \n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = batch_size \n",
    " \n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "\n",
    "    MSE = (sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])/ m)    \n",
    "\n",
    "    while not converged:        \n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        temp0 = t0 - learn_rate * grad0\n",
    "        temp1 = t1 - learn_rate * grad1\n",
    "    \n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        MSE_New = (sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) / m)\n",
    "\n",
    "        if abs(MSE - MSE_New ) <= conv_threshold:\n",
    "            print ('Converged, iterations: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        MSE = MSE_New   \n",
    "        iter += 1 \n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print ('Max interactions reached')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Inter, Coeff = gradient_descent(x = X,y = y,learn_rate=0.00003 ,conv_threshold=1e-8, batch_size=32,max_iter=1500000)\n",
    "    print (\"Gradient Descent Results\")\n",
    "    print (('Intercept = %s Coefficient = %s') %(Inter, Coeff)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation Test split      \n",
    "\n",
    "import pandas as pd      \n",
    "from sklearn.model_selection import train_test_split              \n",
    "                        \n",
    "original_data = pd.read_csv(\"mtcars.csv\")                   \n",
    " \n",
    "\n",
    "def data_split(dat,trf = 0.5,vlf=0.25,tsf = 0.25):\n",
    "    nrows = dat.shape[0]    \n",
    "    trnr = int(nrows*trf)\n",
    "    vlnr = int(nrows*vlf)    \n",
    "    \n",
    "    tr_data,rmng = train_test_split(dat,train_size = trnr,random_state=42)\n",
    "    vl_data, ts_data = train_test_split(rmng,train_size = vlnr,random_state=45)  \n",
    "    \n",
    "    return (tr_data,vl_data,ts_data)\n",
    "\n",
    "\n",
    "train_data, validation_data, test_data = data_split(original_data,trf=0.5,vlf=0.25,tsf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   18.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best score: \n",
      " 0.9660130718954248\n",
      "\n",
      " Best parameters set: \n",
      "\n",
      "\tclf__max_depth: 50\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 2\n",
      "\n",
      " Confusion Matrix on Test data \n",
      " [[815  18]\n",
      " [ 17 134]]\n",
      "\n",
      " Test Accuracy \n",
      " 0.9644308943089431\n",
      "\n",
      "Precision Recall f1 table \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       833\n",
      "           1       0.88      0.89      0.88       151\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.93      0.93      0.93       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid search on Decision Trees\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "input_data = pd.read_csv(\"ad.csv\",header=None)                       \n",
    "\n",
    "X_columns = set(input_data.columns.values)\n",
    "y = input_data[len(input_data.columns.values)-1]\n",
    "X_columns.remove(len(input_data.columns.values)-1)\n",
    "X = input_data[list(X_columns)]\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,random_state=33)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "parameters = {\n",
    "    'clf__max_depth': (50,100,150),\n",
    "    'clf__min_samples_split': (2, 3),\n",
    "    'clf__min_samples_leaf': (1, 2, 3)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print ('\\n Best score: \\n', grid_search.best_score_)\n",
    "print ('\\n Best parameters set: \\n')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "print (\"\\n Confusion Matrix on Test data \\n\",confusion_matrix(y_test,y_pred))\n",
    "print (\"\\n Test Accuracy \\n\",accuracy_score(y_test,y_pred))\n",
    "print (\"\\nPrecision Recall f1 table \\n\",classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
