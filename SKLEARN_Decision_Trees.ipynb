{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNA2qXFjgeatFWCCvibDPsl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### DIFFERENT TYPES OF DT\n","\n","**ID3**\n","It was developed by Ross Quinlan in 1986. It is also called Iterative Dichotomiser 3. The main goal of this algorithm is to find those categorical features, for every node, that will yield the largest information gain for categorical targets.\n","It lets the tree to be grown to their maximum size and then to improve the tree’s ability on unseen data, applies a pruning step. The output of this algorithm would be a multiway tree.\n","\n","**C4.5**\n","It is the successor to ID3 and dynamically defines a discrete attribute that partition the continuous attribute value into a discrete set of intervals. That’s the reason it removed the restriction of categorical features. It converts the ID3 trained tree into sets of ‘IF-THEN’ rules.\n","In order to determine the sequence in which these rules should applied, the accuracy of each rule will be evaluated first.\n","\n","**C5.0**\n","It works similar as C4.5 but it uses less memory and build smaller rulesets. It is more accurate than C4.5.\n","\n","**CART**\n","It is called Classification and Regression Trees alsgorithm. It basically generates binary splits by using the features and threshold yielding the largest information gain at each node (called the Gini index).\n","Homogeneity depends upon Gini index, higher the value of Gini index, higher would be the homogeneity. It is like C4.5 algorithm, but, the difference is that it does not compute rule sets and does not support numerical target variables (regression) as well."],"metadata":{"id":"49JoJ34vT6nN"}},{"cell_type":"markdown","source":["### Classification with Decision Trees"],"metadata":{"id":"1FWrx5E_USqn"}},{"cell_type":"markdown","source":["**Implementation Example**\n","\n","The Python script below will use sklearn.tree.DecisionTreeClassifier module to construct a classifier for predicting male or female from our data set having 25 samples and two features namely ‘height’ and ‘length of hair’:"],"metadata":{"id":"4dv_Oq7aUmrK"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn import tree\n","from sklearn.model_selection import train_test_split\n","\n","X=[[165,19],[175,32],[136,35],[174,65],[141,28],[176,15],[131,32],[166,6],[128,32],[179,10],[136,34],[186,2],[126,25],[176,28],[112,38],[169,9],[171,36],[116,25],[196,25], [196,38], [126,40], [197,20], [150,25], [140,32],[136,35]]\n","Y=['Man','Woman','Woman','Man','Woman','Man','Woman','Man','Woman','Man','Woman','Man','Woman','Woman','Woman','Man','Woman','Woman','Man', 'Woman', 'Woman', 'Man', 'Man', 'Woman', 'Woman']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n","DTclf = tree.DecisionTreeClassifier()\n","DTclf = DTclf.fit(X_train, y_train)\n","prediction = DTclf.predict([[135, 29]])\n","print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LspsUvGUeQc","executionInfo":{"status":"ok","timestamp":1688783453527,"user_tz":-330,"elapsed":398,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"b763ef29-fa67-477a-f2b0-2e7b64889db8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['Woman']\n"]}]},{"cell_type":"markdown","source":["We can also predict the probability of each class by using following python predict_proba() method as follows:"],"metadata":{"id":"bdTUb1XRVx_k"}},{"cell_type":"code","source":["prediction = DTclf.predict_proba([[135,29]])\n","print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1C3g-ozRUpqS","executionInfo":{"status":"ok","timestamp":1688783503879,"user_tz":-330,"elapsed":6,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"a896bad4-0ffc-4d70-b765-0bc29141588c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1.]]\n"]}]},{"cell_type":"markdown","source":["### Regression with Decision Trees"],"metadata":{"id":"D2SbPlc5V5rJ"}},{"cell_type":"markdown","source":["Implementation Example\n","\n","The fit() method in Decision tree regression model will take floating point values of y. let’s see a simple implementation example by using Sklearn.tree.DecisionTreeRegressor:"],"metadata":{"id":"RpMCm0zVWSFI"}},{"cell_type":"code","source":["from sklearn import tree\n","X = [[1, 1], [5, 5]]\n","y = [0.1, 1.5]\n","DTreg = tree.DecisionTreeRegressor()\n","DTreg = DTreg.fit(X, y)"],"metadata":{"id":"K-EOSfg8V9t0","executionInfo":{"status":"ok","timestamp":1688783668673,"user_tz":-330,"elapsed":2,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Once fitted, we can use this regression model to make prediction as follows:"],"metadata":{"id":"jruTTk_cWetn"}},{"cell_type":"code","source":["DTreg.predict([[4, 5]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZOfcMS0WfWl","executionInfo":{"status":"ok","timestamp":1688783687356,"user_tz":-330,"elapsed":4,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"27c3d88e-275e-4f16-d01b-1969627b98ac"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.5])"]},"metadata":{},"execution_count":6}]}]}