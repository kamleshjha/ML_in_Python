{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1h4LvQVEni6Dk7ixrcw-tEjjbHFet5qMm","authorship_tag":"ABX9TyNgeCixIl/ThGSJk37C17BF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### The Random Forest algorithm\n","\n","For each feature under consideration, it computes the locally optimal feature/split combination. In Random forest, each decision tree in the ensemble is built from a sample drawn with replacement from the training set and then gets the prediction from each of them and finally selects the best solution by means of voting. It can be used for both classification as well as regression tasks"],"metadata":{"id":"XoqfjbrJXMYK"}},{"cell_type":"markdown","source":["#### **Classification with Random Forest**\n","\n","For creating a random forest classifier, the Scikit-learn module provides sklearn.ensemble.RandomForestClassifier. While building random forest classifier, the main parameters this module uses are ‘max_features’ and ‘n_estimators’.\n","Here, ‘max_features’ is the size of the random subsets of features to consider when splitting a node. If we choose this parameter’s value to none then it will consider all the features rather than a random subset. On the other hand, n_estimators are the number of trees in the forest. The higher the number of trees, the better the result will be. But it will take longer to compute also."],"metadata":{"id":"7iVFq_ZfXaOT"}},{"cell_type":"markdown","source":["**Implementation example**\n","\n","In the following example, we are building a random forest classifier by using sklearn.ensemble.RandomForestClassifier and also checking its accuracy also by using cross_val_score module."],"metadata":{"id":"nftpd_gtXtwS"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"2RFdebc3W7JV","executionInfo":{"status":"ok","timestamp":1688784099085,"user_tz":-330,"elapsed":1506,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}}},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import make_blobs\n","from sklearn.ensemble import RandomForestClassifier\n","X, Y = make_blobs(n_samples=10000, n_features=10, centers=100,random_state=0)"]},{"cell_type":"code","source":["RFclf = RandomForestClassifier(n_estimators=10,max_depth=None,min_samples_split=2, random_state=0)\n","scores = cross_val_score(RFclf, X, Y, cv=5)\n","scores.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhNYFZNQYBTG","executionInfo":{"status":"ok","timestamp":1688784172189,"user_tz":-330,"elapsed":2698,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"2274a387-a91c-43c7-8eda-78605d549673"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9997"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["We can also use the sklearn dataset to build Random Forest classifier. As in the following example we are using iris dataset. We will also find its accuracy score and confusion matrix."],"metadata":{"id":"bOeD0kf0Ybc0"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","\n","path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n","headernames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n","dataset = pd.read_csv(path, names=headernames)\n","X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, 4].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n","RFclf = RandomForestClassifier(n_estimators=50)\n","RFclf.fit(X_train, y_train)\n","y_pred = RFclf.predict(X_test)"],"metadata":{"id":"asIAnI7rYcWN","executionInfo":{"status":"ok","timestamp":1688784296072,"user_tz":-330,"elapsed":1117,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["result = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(result)\n","result1 = classification_report(y_test, y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5o-aUVpY-X8","executionInfo":{"status":"ok","timestamp":1688784361875,"user_tz":-330,"elapsed":621,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"be3218ed-6292-491f-dae3-c5ea7cc0a782"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n","[[12  0  0]\n"," [ 0 18  1]\n"," [ 0  1 13]]\n"]}]},{"cell_type":"code","source":["print(\"Classification Report:\",)\n","print (result1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yx2MGf5zZBU2","executionInfo":{"status":"ok","timestamp":1688784366821,"user_tz":-330,"elapsed":494,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"ef4f042b-5340-4060-95c4-283c2d2a7813"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report:\n","                 precision    recall  f1-score   support\n","\n","    Iris-setosa       1.00      1.00      1.00        12\n","Iris-versicolor       0.95      0.95      0.95        19\n"," Iris-virginica       0.93      0.93      0.93        14\n","\n","       accuracy                           0.96        45\n","      macro avg       0.96      0.96      0.96        45\n","   weighted avg       0.96      0.96      0.96        45\n","\n"]}]},{"cell_type":"code","source":["result2 = accuracy_score(y_test,y_pred)\n","print(\"Accuracy:\",result2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCNnJYHDZDiw","executionInfo":{"status":"ok","timestamp":1688784371879,"user_tz":-330,"elapsed":411,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"9fed4a22-a826-4092-de55-e881f88a29bc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9555555555555556\n"]}]},{"cell_type":"markdown","source":["#### **Regression with Random Forest**\n","\n","For creating a random forest regression, the Scikit-learn module provides sklearn.ensemble.RandomForestRegressor. While building random forest regressor, it will use the same parameters as used by sklearn.ensemble.RandomForestClassifier."],"metadata":{"id":"Sa9Xvw9_ZSUH"}},{"cell_type":"markdown","source":["Implementation example\n","\n","In the following example, we are building a random forest regressor by using sklearn.ensemble.RandomForestregressor and also predicting for new values by using predict() method."],"metadata":{"id":"Bkd9o3BAZZsD"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","X, y = make_regression(n_features=10, n_informative=2,random_state=0, shuffle=False)\n","RFregr = RandomForestRegressor(max_depth=10,random_state=0,n_estimators=100)\n","RFregr.fit(X, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"LNplnuWbZXi1","executionInfo":{"status":"ok","timestamp":1688784471046,"user_tz":-330,"elapsed":457,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"6bfe4986-8a41-4799-caa0-ab638754839b"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestRegressor(max_depth=10, random_state=0)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=0)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Once fitted we can predict from regression model as follows:"],"metadata":{"id":"euYysSe6Zj2a"}},{"cell_type":"code","source":["print(RFregr.predict([[0, 2, 3, 0, 1, 1, 1, 1, 2, 2]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0jgEaUZZkU0","executionInfo":{"status":"ok","timestamp":1688784495993,"user_tz":-330,"elapsed":439,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"c92fde68-8327-4896-eb29-4b83e8d37b37"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[98.47729198]\n"]}]},{"cell_type":"markdown","source":["### EXTRA TREES\n","\n","For each feature under consideration, it selects a random value for the split. The benefit of using extra tree methods is that it allows to reduce the variance of the model a bit more. The disadvantage of using these methods is that it slightly increases the bias."],"metadata":{"id":"vZ-0ZvSRaDsD"}},{"cell_type":"markdown","source":["#### EXTRA TREES METHOD\n","\n","For creating a classifier using Extra-tree method, the Scikit-learn module provides sklearn.ensemble.ExtraTreesClassifier. It uses the same parameters as used by sklearn.ensemble.RandomForestClassifier. The only difference is in the way, discussed above, they build trees."],"metadata":{"id":"tDBND-mSaShs"}},{"cell_type":"markdown","source":["Implementation example\n","\n","In the following example, we are building a random forest classifier by using sklearn.ensemble.ExtraTreeClassifier and also checking its accuracy by using cross_val_score module."],"metadata":{"id":"RQEKRNBeaaBg"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import make_blobs\n","from sklearn.ensemble import ExtraTreesClassifier\n","X, y = make_blobs(n_samples=10000, n_features=10, centers=100,random_state=0)\n","ETclf = ExtraTreesClassifier(n_estimators=10,max_depth=None,min_samples_split=10, random_state=0)\n","scores = cross_val_score(ETclf, X, y, cv=5)\n","scores.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXf15mEKaYID","executionInfo":{"status":"ok","timestamp":1688784727306,"user_tz":-330,"elapsed":696,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"25f30741-ee0d-45bc-a16b-04ad3a024b24"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["We can also use the sklearn dataset to build classifier using Extra-Tree method. As in the following example we are using Pima-Indian dataset."],"metadata":{"id":"jeoqT2cNail6"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","path = \"/content/drive/MyDrive/Colab Notebooks/data/pima-indians-diabetes.csv\"\n","header_names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","\n","data = pd.read_csv(path, names=header_names)\n","array = data.values\n","X = array[:, 0:8]\n","Y = array[:, 8]\n","\n","seed = 7\n","kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n","num_trees = 150\n","max_features = 5\n","\n","ETclf = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n","\n","results = cross_val_score(ETclf, X, Y, cv=kfold)\n","\n","print(results.mean())\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EeMfTo1Tafrk","executionInfo":{"status":"error","timestamp":1688785408292,"user_tz":-330,"elapsed":4,"user":{"displayName":"Kamlesh Jha","userId":"17539185982158631763"}},"outputId":"bd8386a3-c0ce-490d-e3f1-04aeaee41b94"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-a4e2bfc90b35>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mETclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mETclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# For callabe scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n9 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '6,148,72,35,0,33.6,0.627,50,1'\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '1,85,66,29,0,26.6,0.351,31,0'\n"]}]}]}